{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "나이브 베이즈 분류기는 선형 모델과 매우 유사. Logistic Reg. 이나 Linear SVC 같은 선형분류기보다 훈련속도가 빠른 편이지만, 일반화 성능이 조금 낮음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "나이브 베인즈 분류기가 효과적인 이유는 각 특성을 개별로 취급해 파라미터를 학습하고 각 특서에서 클래스별 통계를 단순하게 취합하기 때문.\n",
    "sklearn에 구현된 naive bayes 분류기는 GaussianNB, BernoulliNB, MultinomialNB 이렇게 세가지. GaussianNB는 연속적인 어떤 데이터에도 적용할 수 있고, BernoulliNB는 이진 데이터, MultinnomialNB는 카운트 데이터(문장에 나타난 단어의 횟수데이터 같은 정수카운트)에 적용됨. BernoulliNB, MultinomialNB는 대부분 텍스트 분석에 사용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BernoulliNB 분류기는 각 클래스의 특성 중 0이 아닌 것이 몇 개인지 셈.ㅕ \n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[0,1,0,1],[1,0,1,1],[0,0,0,1],[1,0,1,0]]) # 이진 특성을 4개 가진 데이터 포인트 4개를 생성.\n",
    "y = np.array([0,1,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "특성 카운트:\n",
      " {0: array([0, 1, 0, 2]), 1: array([2, 0, 2, 1])}\n"
     ]
    }
   ],
   "source": [
    "counts = {}\n",
    "for label in np.unique(y):\n",
    "    # 각 클래스에 대해 반복\n",
    "    # 특성마다 1 이 나타난 횟수를 센다.\n",
    "    counts[label] = X[y==label].sum(axis=0)\n",
    "print(\"특성 카운트:\\n\", counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 1],\n",
       "       [1, 0, 1, 1],\n",
       "       [0, 0, 0, 1],\n",
       "       [1, 0, 1, 0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 1],\n",
       "       [1, 0, 1, 0]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[y==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 1],\n",
       "       [0, 0, 0, 1]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[y==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB는 훈련과 예측 속도가 빠르며 훈련과정을 이해하기 쉬움. 희소한 고차원 데이터에서 잘 작동. 비교적 매개변수에 민감하지않음. 산형 모델로는 학습 시간이 너무 오래 걸리는 매우 큰 데이터셋에는 나이브 베이즈 모델이 종종 사용됨."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
